{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version is: 1.26.4\n",
      "torch version is: 2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "print(\"numpy version is:\", np.__version__)\n",
    "print(\"torch version is:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor(3, 2)\n",
    "a.zero_()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "n = np.zeros(shape=(2,3))\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "m = torch.tensor(n, dtype=torch.int32)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "[Parameter containing:\n",
      "tensor([[-0.0380, -0.2205],\n",
      "        [ 0.1677, -0.0954],\n",
      "        [ 0.2849,  0.0397],\n",
      "        [ 0.3442, -0.3085],\n",
      "        [-0.2297, -0.4714]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2561, -0.6667,  0.2437, -0.5131,  0.0876], device='cuda:0',\n",
      "       requires_grad=True)]\n",
      "tensor([1., 2.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "device = torch.device('cuda', 0)\n",
    "print(device)\n",
    "l = nn.Linear(2,5).to(device)\n",
    "v = torch.tensor([1,2], dtype=torch.float32).to(device)\n",
    "print([i for i in l.parameters()])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4055,  1.3871,  0.1770, -0.9062,  0.6648], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=5, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=5, out_features=20, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (5): Dropout(p=0.3, inplace=False)\n",
      "  (6): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "s = nn.Sequential(\n",
    "    nn.Linear(2, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5,20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Softmax(dim=1)\n",
    ").to(device)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1411, 0.1268, 0.1070, 0.0993, 0.1025, 0.0630, 0.1025, 0.0774, 0.1025,\n",
       "         0.0779]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(torch.tensor([[1,2]], dtype=torch.float32).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitriy/notebook/jupyter_notebook/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment AirRaid-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"AirRaid-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 255, (250, 160, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import gymnasium as gym\n",
    "#import gymnasium.spaces as gym.spaces\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "log = gym.logger\n",
    "log.set_level(gym.logger.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "LATENT_VECTOR_SIZE = 100\n",
    "DISCR_FILTERS = 64\n",
    "GENER_FILTERS = 64\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "REPORT_EVERY_ITER = 100\n",
    "SAVE_IMAGE_EVERY_ITER = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for observation preprocessing\n",
    "\n",
    "class PreprocessingObservationWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Preprocessing of input numpy array:\n",
    "    1. resize image into predefined size\n",
    "    2. move color channel axis to a first place\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(PreprocessingObservationWrapper, self).__init__(*args)\n",
    "        # Why do we need it?\n",
    "        assert isinstance(self.observation_space, gym.spaces.Box) \n",
    "        old_space = self.observation_space\n",
    "        self.observation_space = gym.spaces.Box(self.observation(old_space.low), self.observation(old_space.high),\n",
    "                                                dtype=np.float32)\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        new_observation = cv2.resize(observation, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        new_observation = np.moveaxis(new_observation, -1, 0)\n",
    "        return new_observation.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for discriminator\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # this pipe converges image into the single number\n",
    "        self.conv_pipe = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape[0], out_channels=DISCR_FILTERS,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS, out_channels=DISCR_FILTERS*2,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 2, out_channels=DISCR_FILTERS * 4,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 4, out_channels=DISCR_FILTERS * 8,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 8, out_channels=1,\n",
    "                      kernel_size=4, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_pipe(x)\n",
    "        return conv_out.view(-1, 1).squeeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for generator\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, output_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        # pipe deconvolves input vector into (3, 64, 64) image\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=LATENT_VECTOR_SIZE, out_channels=GENER_FILTERS * 8,\n",
    "                               kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 8, out_channels=GENER_FILTERS * 4,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 4, out_channels=GENER_FILTERS * 2,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 2, out_channels=GENER_FILTERS,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS, out_channels=output_shape[0],\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a batch of observations to pass to discriminator\n",
    "\n",
    "def iterate_batches(envs, batch_size=BATCH_SIZE):\n",
    "    batch = [e.reset()[0] for e in envs]\n",
    "    env_gen = iter(lambda: random.choice(envs), None)\n",
    "\n",
    "    while True:\n",
    "        e = next(env_gen)\n",
    "        obs, reward, terminated, truncated, _ = e.step(e.action_space.sample())\n",
    "        if np.mean(obs) > 0.01:\n",
    "            batch.append(obs)\n",
    "        if len(batch) == batch_size:\n",
    "            # Normalising input between -1 to 1\n",
    "            batch_np = np.array(batch, dtype=np.float32) * 2.0 / 255.0 - 1.0\n",
    "            yield torch.tensor(batch_np)\n",
    "            batch.clear()\n",
    "        if terminated or truncated:\n",
    "            e.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "envs = [PreprocessingObservationWrapper(gym.make(name)) for name in ('Breakout-v0', )]\n",
    "input_shape = envs[0].observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_discr = Discriminator(input_shape=input_shape).to(device)\n",
    "net_gener = Generator(output_shape=input_shape).to(device)\n",
    "\n",
    "objective = nn.BCELoss()\n",
    "gen_optimizer = optim.Adam(params=net_gener.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "dis_optimizer = optim.Adam(params=net_discr.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "writer = SummaryWriter()\n",
    "\n",
    "gen_losses = []\n",
    "dis_losses = []\n",
    "iter_no = 0\n",
    "\n",
    "true_labels_v = torch.ones(BATCH_SIZE, dtype=torch.float32, device=device)\n",
    "fake_labels_v = torch.zeros(BATCH_SIZE, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Iter 100: gen_loss=5.233e+00, dis_loss=4.609e-02\n",
      "INFO: Iter 200: gen_loss=6.763e+00, dis_loss=2.808e-03\n",
      "INFO: Iter 300: gen_loss=7.284e+00, dis_loss=1.467e-03\n",
      "INFO: Iter 400: gen_loss=7.573e+00, dis_loss=1.048e-03\n",
      "INFO: Iter 500: gen_loss=7.769e+00, dis_loss=8.226e-04\n",
      "INFO: Iter 600: gen_loss=8.044e+00, dis_loss=6.051e-04\n",
      "INFO: Iter 700: gen_loss=8.347e+00, dis_loss=4.337e-04\n",
      "INFO: Iter 800: gen_loss=8.551e+00, dis_loss=3.661e-04\n",
      "INFO: Iter 900: gen_loss=8.766e+00, dis_loss=3.031e-04\n",
      "INFO: Iter 1000: gen_loss=8.168e+00, dis_loss=3.476e-01\n",
      "INFO: Iter 1100: gen_loss=4.498e+00, dis_loss=3.942e-01\n",
      "INFO: Iter 1200: gen_loss=5.207e+00, dis_loss=1.310e-01\n",
      "INFO: Iter 1300: gen_loss=6.202e+00, dis_loss=7.613e-03\n",
      "INFO: Iter 1400: gen_loss=6.641e+00, dis_loss=3.284e-03\n",
      "INFO: Iter 1500: gen_loss=7.159e+00, dis_loss=1.912e-03\n",
      "INFO: Iter 1600: gen_loss=7.680e+00, dis_loss=1.025e-03\n",
      "INFO: Iter 1700: gen_loss=7.864e+00, dis_loss=7.844e-04\n",
      "INFO: Iter 1800: gen_loss=8.170e+00, dis_loss=5.454e-04\n",
      "INFO: Iter 1900: gen_loss=8.351e+00, dis_loss=4.416e-04\n",
      "INFO: Iter 2000: gen_loss=8.619e+00, dis_loss=3.274e-04\n",
      "INFO: Iter 2100: gen_loss=8.697e+00, dis_loss=2.892e-04\n",
      "INFO: Iter 2200: gen_loss=8.774e+00, dis_loss=2.915e-04\n",
      "INFO: Iter 2300: gen_loss=9.020e+00, dis_loss=2.172e-04\n",
      "INFO: Iter 2400: gen_loss=9.148e+00, dis_loss=2.064e-04\n",
      "INFO: Iter 2500: gen_loss=9.252e+00, dis_loss=1.882e-04\n",
      "INFO: Iter 2600: gen_loss=9.256e+00, dis_loss=1.925e-04\n",
      "INFO: Iter 2700: gen_loss=9.387e+00, dis_loss=1.880e-04\n",
      "INFO: Iter 2800: gen_loss=9.740e+00, dis_loss=1.289e-04\n",
      "INFO: Iter 2900: gen_loss=9.604e+00, dis_loss=2.050e-04\n",
      "INFO: Iter 3000: gen_loss=1.006e+01, dis_loss=9.040e-05\n",
      "INFO: Iter 3100: gen_loss=1.019e+01, dis_loss=7.471e-05\n",
      "INFO: Iter 3200: gen_loss=1.009e+01, dis_loss=8.838e-05\n",
      "INFO: Iter 3300: gen_loss=1.034e+01, dis_loss=6.764e-05\n",
      "INFO: Iter 3400: gen_loss=1.042e+01, dis_loss=6.159e-05\n",
      "INFO: Iter 3500: gen_loss=1.044e+01, dis_loss=6.228e-05\n",
      "INFO: Iter 3600: gen_loss=1.045e+01, dis_loss=6.626e-05\n",
      "INFO: Iter 3700: gen_loss=1.061e+01, dis_loss=5.688e-05\n",
      "INFO: Iter 3800: gen_loss=1.085e+01, dis_loss=4.289e-05\n",
      "INFO: Iter 3900: gen_loss=1.041e+01, dis_loss=7.287e-05\n",
      "INFO: Iter 4000: gen_loss=1.020e+01, dis_loss=1.019e-04\n",
      "INFO: Iter 4100: gen_loss=7.199e+00, dis_loss=3.234e-01\n",
      "INFO: Iter 4200: gen_loss=4.590e+00, dis_loss=1.978e-01\n",
      "INFO: Iter 4300: gen_loss=4.040e+00, dis_loss=5.218e-01\n",
      "INFO: Iter 4400: gen_loss=3.116e+00, dis_loss=7.893e-01\n",
      "INFO: Iter 4500: gen_loss=3.099e+00, dis_loss=7.567e-01\n",
      "INFO: Iter 4600: gen_loss=3.670e+00, dis_loss=6.775e-01\n",
      "INFO: Iter 4700: gen_loss=3.724e+00, dis_loss=5.613e-01\n",
      "INFO: Iter 4800: gen_loss=4.291e+00, dis_loss=4.131e-01\n",
      "INFO: Iter 4900: gen_loss=3.960e+00, dis_loss=4.501e-01\n",
      "INFO: Iter 5000: gen_loss=3.626e+00, dis_loss=3.667e-01\n",
      "INFO: Iter 5100: gen_loss=4.494e+00, dis_loss=2.077e-01\n",
      "INFO: Iter 5200: gen_loss=4.560e+00, dis_loss=3.667e-01\n",
      "INFO: Iter 5300: gen_loss=3.548e+00, dis_loss=4.142e-01\n",
      "INFO: Iter 5400: gen_loss=4.690e+00, dis_loss=1.826e-01\n",
      "INFO: Iter 5500: gen_loss=5.307e+00, dis_loss=9.016e-02\n",
      "INFO: Iter 5600: gen_loss=5.120e+00, dis_loss=1.565e-01\n",
      "INFO: Iter 5700: gen_loss=4.466e+00, dis_loss=2.761e-01\n",
      "INFO: Iter 5800: gen_loss=5.014e+00, dis_loss=1.885e-01\n",
      "INFO: Iter 5900: gen_loss=4.496e+00, dis_loss=3.105e-01\n",
      "INFO: Iter 6000: gen_loss=5.031e+00, dis_loss=1.539e-01\n",
      "INFO: Iter 6100: gen_loss=5.117e+00, dis_loss=2.010e-01\n",
      "INFO: Iter 6200: gen_loss=3.816e+00, dis_loss=4.746e-01\n",
      "INFO: Iter 6300: gen_loss=4.333e+00, dis_loss=2.118e-01\n",
      "INFO: Iter 6400: gen_loss=5.862e+00, dis_loss=1.023e-01\n",
      "INFO: Iter 6500: gen_loss=4.906e+00, dis_loss=3.381e-01\n",
      "INFO: Iter 6600: gen_loss=6.030e+00, dis_loss=3.016e-01\n",
      "INFO: Iter 6700: gen_loss=4.787e+00, dis_loss=1.942e-01\n",
      "INFO: Iter 6800: gen_loss=4.641e+00, dis_loss=1.990e-01\n",
      "INFO: Iter 6900: gen_loss=5.442e+00, dis_loss=3.464e-02\n",
      "INFO: Iter 7000: gen_loss=6.233e+00, dis_loss=1.038e-01\n",
      "INFO: Iter 7100: gen_loss=5.615e+00, dis_loss=2.056e-02\n",
      "INFO: Iter 7200: gen_loss=5.530e+00, dis_loss=1.118e-02\n",
      "INFO: Iter 7300: gen_loss=5.700e+00, dis_loss=1.001e-02\n",
      "INFO: Iter 7400: gen_loss=5.627e+00, dis_loss=1.112e-02\n",
      "INFO: Iter 7500: gen_loss=6.070e+00, dis_loss=5.592e-03\n",
      "INFO: Iter 7600: gen_loss=6.434e+00, dis_loss=3.754e-03\n",
      "INFO: Iter 7700: gen_loss=6.768e+00, dis_loss=2.700e-03\n",
      "INFO: Iter 7800: gen_loss=6.814e+00, dis_loss=2.475e-03\n",
      "INFO: Iter 7900: gen_loss=7.133e+00, dis_loss=1.570e-03\n",
      "INFO: Iter 8000: gen_loss=7.431e+00, dis_loss=1.489e-03\n",
      "INFO: Iter 8100: gen_loss=7.628e+00, dis_loss=1.012e-03\n",
      "INFO: Iter 8200: gen_loss=7.781e+00, dis_loss=1.036e-03\n",
      "INFO: Iter 8300: gen_loss=7.869e+00, dis_loss=9.339e-04\n",
      "INFO: Iter 8400: gen_loss=8.155e+00, dis_loss=7.226e-04\n",
      "INFO: Iter 8500: gen_loss=8.280e+00, dis_loss=5.813e-04\n",
      "INFO: Iter 8600: gen_loss=8.360e+00, dis_loss=5.756e-04\n",
      "INFO: Iter 8700: gen_loss=8.438e+00, dis_loss=5.177e-04\n",
      "INFO: Iter 8800: gen_loss=8.559e+00, dis_loss=4.053e-04\n",
      "INFO: Iter 8900: gen_loss=8.668e+00, dis_loss=3.748e-04\n",
      "INFO: Iter 9000: gen_loss=8.707e+00, dis_loss=3.495e-04\n",
      "INFO: Iter 9100: gen_loss=8.952e+00, dis_loss=2.708e-04\n",
      "INFO: Iter 9200: gen_loss=8.998e+00, dis_loss=2.741e-04\n",
      "INFO: Iter 9300: gen_loss=9.148e+00, dis_loss=2.452e-04\n",
      "INFO: Iter 9400: gen_loss=9.335e+00, dis_loss=2.255e-04\n",
      "INFO: Iter 9500: gen_loss=9.368e+00, dis_loss=2.234e-04\n",
      "INFO: Iter 9600: gen_loss=9.511e+00, dis_loss=1.442e-04\n",
      "INFO: Iter 9700: gen_loss=9.692e+00, dis_loss=1.193e-04\n",
      "INFO: Iter 9800: gen_loss=9.855e+00, dis_loss=1.076e-04\n",
      "INFO: Iter 9900: gen_loss=9.997e+00, dis_loss=9.834e-05\n",
      "INFO: Iter 10000: gen_loss=1.032e+01, dis_loss=5.974e-05\n",
      "INFO: Iter 10100: gen_loss=1.017e+01, dis_loss=9.696e-05\n",
      "INFO: Iter 10200: gen_loss=1.049e+01, dis_loss=6.173e-05\n",
      "INFO: Iter 10300: gen_loss=1.030e+01, dis_loss=8.357e-05\n",
      "INFO: Iter 10400: gen_loss=1.036e+01, dis_loss=6.904e-05\n",
      "INFO: Iter 10500: gen_loss=1.041e+01, dis_loss=6.670e-05\n",
      "INFO: Iter 10600: gen_loss=1.009e+01, dis_loss=1.299e-04\n",
      "INFO: Iter 10700: gen_loss=1.067e+01, dis_loss=4.849e-05\n",
      "INFO: Iter 10800: gen_loss=1.058e+01, dis_loss=7.579e-05\n",
      "INFO: Iter 10900: gen_loss=1.072e+01, dis_loss=6.178e-05\n",
      "INFO: Iter 11000: gen_loss=1.078e+01, dis_loss=5.145e-05\n",
      "INFO: Iter 11100: gen_loss=1.054e+01, dis_loss=8.711e-05\n",
      "INFO: Iter 11200: gen_loss=1.045e+01, dis_loss=6.831e-05\n",
      "INFO: Iter 11300: gen_loss=1.056e+01, dis_loss=7.516e-05\n",
      "INFO: Iter 11400: gen_loss=1.075e+01, dis_loss=4.167e-05\n",
      "INFO: Iter 11500: gen_loss=4.175e+01, dis_loss=3.670e+01\n",
      "INFO: Iter 11600: gen_loss=5.229e+01, dis_loss=4.974e+01\n",
      "INFO: Iter 11700: gen_loss=5.229e+01, dis_loss=4.970e+01\n",
      "INFO: Iter 11800: gen_loss=5.229e+01, dis_loss=4.969e+01\n",
      "INFO: Iter 11900: gen_loss=5.229e+01, dis_loss=4.975e+01\n",
      "INFO: Iter 12000: gen_loss=5.229e+01, dis_loss=4.976e+01\n",
      "INFO: Iter 12100: gen_loss=5.229e+01, dis_loss=4.980e+01\n",
      "INFO: Iter 12200: gen_loss=5.229e+01, dis_loss=4.975e+01\n",
      "INFO: Iter 12300: gen_loss=5.229e+01, dis_loss=4.975e+01\n",
      "INFO: Iter 12400: gen_loss=5.229e+01, dis_loss=4.980e+01\n",
      "INFO: Iter 12500: gen_loss=5.229e+01, dis_loss=4.972e+01\n",
      "INFO: Iter 12600: gen_loss=5.229e+01, dis_loss=4.973e+01\n",
      "INFO: Iter 12700: gen_loss=5.229e+01, dis_loss=4.988e+01\n",
      "INFO: Iter 12800: gen_loss=5.229e+01, dis_loss=4.967e+01\n",
      "INFO: Iter 12900: gen_loss=5.229e+01, dis_loss=4.978e+01\n",
      "INFO: Iter 13000: gen_loss=5.229e+01, dis_loss=4.980e+01\n",
      "INFO: Iter 13100: gen_loss=5.229e+01, dis_loss=4.979e+01\n",
      "INFO: Iter 13200: gen_loss=5.229e+01, dis_loss=4.974e+01\n",
      "INFO: Iter 13300: gen_loss=5.229e+01, dis_loss=4.979e+01\n",
      "INFO: Iter 13400: gen_loss=5.229e+01, dis_loss=4.975e+01\n",
      "INFO: Iter 13500: gen_loss=5.229e+01, dis_loss=4.979e+01\n",
      "INFO: Iter 13600: gen_loss=5.229e+01, dis_loss=4.979e+01\n",
      "INFO: Iter 13700: gen_loss=5.229e+01, dis_loss=4.976e+01\n",
      "INFO: Iter 13800: gen_loss=5.229e+01, dis_loss=4.971e+01\n",
      "INFO: Iter 13900: gen_loss=5.229e+01, dis_loss=4.974e+01\n",
      "INFO: Iter 14000: gen_loss=5.229e+01, dis_loss=4.974e+01\n",
      "INFO: Iter 14100: gen_loss=5.229e+01, dis_loss=4.968e+01\n",
      "INFO: Iter 14200: gen_loss=5.229e+01, dis_loss=4.979e+01\n",
      "INFO: Iter 14300: gen_loss=5.229e+01, dis_loss=4.970e+01\n",
      "INFO: Iter 14400: gen_loss=5.229e+01, dis_loss=4.985e+01\n",
      "INFO: Iter 14500: gen_loss=5.229e+01, dis_loss=4.974e+01\n",
      "INFO: Iter 14600: gen_loss=5.229e+01, dis_loss=4.988e+01\n",
      "INFO: Iter 14700: gen_loss=5.229e+01, dis_loss=4.978e+01\n",
      "INFO: Iter 14800: gen_loss=5.229e+01, dis_loss=4.970e+01\n",
      "INFO: Iter 14900: gen_loss=5.229e+01, dis_loss=4.969e+01\n",
      "INFO: Iter 15000: gen_loss=5.229e+01, dis_loss=4.966e+01\n",
      "INFO: Iter 15100: gen_loss=5.229e+01, dis_loss=4.973e+01\n",
      "INFO: Iter 15200: gen_loss=5.229e+01, dis_loss=4.976e+01\n",
      "INFO: Iter 15300: gen_loss=5.229e+01, dis_loss=4.982e+01\n",
      "INFO: Iter 15400: gen_loss=5.229e+01, dis_loss=4.974e+01\n",
      "INFO: Iter 15500: gen_loss=5.229e+01, dis_loss=4.978e+01\n",
      "INFO: Iter 15600: gen_loss=5.229e+01, dis_loss=4.979e+01\n",
      "INFO: Iter 15700: gen_loss=5.229e+01, dis_loss=4.981e+01\n",
      "INFO: Iter 15800: gen_loss=5.229e+01, dis_loss=4.972e+01\n",
      "INFO: Iter 15900: gen_loss=5.229e+01, dis_loss=4.982e+01\n",
      "INFO: Iter 16000: gen_loss=5.229e+01, dis_loss=4.979e+01\n",
      "INFO: Iter 16100: gen_loss=5.229e+01, dis_loss=4.970e+01\n",
      "INFO: Iter 16200: gen_loss=5.229e+01, dis_loss=4.968e+01\n",
      "INFO: Iter 16300: gen_loss=5.229e+01, dis_loss=4.978e+01\n",
      "INFO: Iter 16400: gen_loss=5.229e+01, dis_loss=4.977e+01\n",
      "INFO: Iter 16500: gen_loss=5.229e+01, dis_loss=4.971e+01\n",
      "INFO: Iter 16600: gen_loss=5.229e+01, dis_loss=4.985e+01\n",
      "INFO: Iter 16700: gen_loss=5.229e+01, dis_loss=4.981e+01\n",
      "INFO: Iter 16800: gen_loss=5.229e+01, dis_loss=4.981e+01\n",
      "INFO: Iter 16900: gen_loss=5.229e+01, dis_loss=4.980e+01\n",
      "INFO: Iter 17000: gen_loss=5.229e+01, dis_loss=4.986e+01\n",
      "INFO: Iter 17100: gen_loss=5.229e+01, dis_loss=4.976e+01\n",
      "INFO: Iter 17200: gen_loss=5.229e+01, dis_loss=4.976e+01\n",
      "INFO: Iter 17300: gen_loss=5.229e+01, dis_loss=4.969e+01\n",
      "INFO: Iter 17400: gen_loss=5.229e+01, dis_loss=4.973e+01\n",
      "INFO: Iter 17500: gen_loss=5.229e+01, dis_loss=4.979e+01\n",
      "INFO: Iter 17600: gen_loss=5.229e+01, dis_loss=4.977e+01\n",
      "INFO: Iter 17700: gen_loss=5.229e+01, dis_loss=4.983e+01\n",
      "INFO: Iter 17800: gen_loss=5.229e+01, dis_loss=4.981e+01\n",
      "INFO: Iter 17900: gen_loss=5.229e+01, dis_loss=4.980e+01\n",
      "INFO: Iter 18000: gen_loss=5.229e+01, dis_loss=4.975e+01\n",
      "INFO: Iter 18100: gen_loss=5.229e+01, dis_loss=4.983e+01\n",
      "INFO: Iter 18200: gen_loss=5.229e+01, dis_loss=4.972e+01\n",
      "INFO: Iter 18300: gen_loss=5.229e+01, dis_loss=4.987e+01\n",
      "INFO: Iter 18400: gen_loss=5.229e+01, dis_loss=4.973e+01\n",
      "INFO: Iter 18500: gen_loss=5.229e+01, dis_loss=4.975e+01\n",
      "INFO: Iter 18600: gen_loss=5.229e+01, dis_loss=4.979e+01\n",
      "INFO: Iter 18700: gen_loss=5.229e+01, dis_loss=4.981e+01\n",
      "INFO: Iter 18800: gen_loss=5.229e+01, dis_loss=4.985e+01\n",
      "INFO: Iter 18900: gen_loss=5.229e+01, dis_loss=4.982e+01\n",
      "INFO: Iter 19000: gen_loss=5.229e+01, dis_loss=4.979e+01\n",
      "INFO: Iter 19100: gen_loss=5.229e+01, dis_loss=4.972e+01\n",
      "INFO: Iter 19200: gen_loss=5.229e+01, dis_loss=4.977e+01\n",
      "INFO: Iter 19300: gen_loss=5.229e+01, dis_loss=4.973e+01\n",
      "INFO: Iter 19400: gen_loss=5.229e+01, dis_loss=4.977e+01\n",
      "INFO: Iter 19500: gen_loss=5.229e+01, dis_loss=4.979e+01\n",
      "INFO: Iter 19600: gen_loss=5.229e+01, dis_loss=4.981e+01\n",
      "INFO: Iter 19700: gen_loss=5.229e+01, dis_loss=4.982e+01\n",
      "INFO: Iter 19800: gen_loss=5.229e+01, dis_loss=4.978e+01\n",
      "INFO: Iter 19900: gen_loss=5.229e+01, dis_loss=4.975e+01\n",
      "INFO: Iter 20000: gen_loss=5.229e+01, dis_loss=4.979e+01\n",
      "INFO: Iter 20100: gen_loss=5.229e+01, dis_loss=4.976e+01\n",
      "INFO: Iter 20200: gen_loss=5.229e+01, dis_loss=4.971e+01\n",
      "INFO: Iter 20300: gen_loss=5.229e+01, dis_loss=4.975e+01\n",
      "INFO: Iter 20400: gen_loss=5.229e+01, dis_loss=4.968e+01\n",
      "INFO: Iter 20500: gen_loss=5.229e+01, dis_loss=4.979e+01\n",
      "INFO: Iter 20600: gen_loss=5.229e+01, dis_loss=4.982e+01\n",
      "INFO: Iter 20700: gen_loss=5.229e+01, dis_loss=4.983e+01\n",
      "INFO: Iter 20800: gen_loss=5.229e+01, dis_loss=4.982e+01\n",
      "INFO: Iter 20900: gen_loss=5.229e+01, dis_loss=4.979e+01\n",
      "INFO: Iter 21000: gen_loss=5.229e+01, dis_loss=4.980e+01\n",
      "INFO: Iter 21100: gen_loss=5.228e+01, dis_loss=4.971e+01\n",
      "INFO: Iter 21200: gen_loss=5.228e+01, dis_loss=4.976e+01\n",
      "INFO: Iter 21300: gen_loss=5.228e+01, dis_loss=4.971e+01\n",
      "INFO: Iter 21400: gen_loss=5.228e+01, dis_loss=4.972e+01\n",
      "INFO: Iter 21500: gen_loss=5.228e+01, dis_loss=4.976e+01\n",
      "INFO: Iter 21600: gen_loss=5.228e+01, dis_loss=4.973e+01\n",
      "INFO: Iter 21700: gen_loss=5.228e+01, dis_loss=4.973e+01\n",
      "INFO: Iter 21800: gen_loss=5.228e+01, dis_loss=4.971e+01\n",
      "INFO: Iter 21900: gen_loss=5.228e+01, dis_loss=4.982e+01\n",
      "INFO: Iter 22000: gen_loss=5.228e+01, dis_loss=4.967e+01\n",
      "INFO: Iter 22100: gen_loss=5.228e+01, dis_loss=4.982e+01\n",
      "INFO: Iter 22200: gen_loss=5.228e+01, dis_loss=4.970e+01\n",
      "INFO: Iter 22300: gen_loss=5.228e+01, dis_loss=4.970e+01\n",
      "INFO: Iter 22400: gen_loss=5.228e+01, dis_loss=4.978e+01\n",
      "INFO: Iter 22500: gen_loss=5.228e+01, dis_loss=4.973e+01\n",
      "INFO: Iter 22600: gen_loss=5.228e+01, dis_loss=4.980e+01\n",
      "INFO: Iter 22700: gen_loss=5.227e+01, dis_loss=4.973e+01\n",
      "INFO: Iter 22800: gen_loss=5.227e+01, dis_loss=4.967e+01\n",
      "INFO: Iter 22900: gen_loss=5.227e+01, dis_loss=4.975e+01\n",
      "INFO: Iter 23000: gen_loss=5.227e+01, dis_loss=4.977e+01\n",
      "INFO: Iter 23100: gen_loss=5.227e+01, dis_loss=4.982e+01\n",
      "INFO: Iter 23200: gen_loss=5.227e+01, dis_loss=4.982e+01\n",
      "INFO: Iter 23300: gen_loss=5.227e+01, dis_loss=4.963e+01\n",
      "INFO: Iter 23400: gen_loss=5.227e+01, dis_loss=4.968e+01\n",
      "INFO: Iter 23500: gen_loss=5.226e+01, dis_loss=4.972e+01\n",
      "INFO: Iter 23600: gen_loss=5.226e+01, dis_loss=4.971e+01\n",
      "INFO: Iter 23700: gen_loss=5.226e+01, dis_loss=4.973e+01\n",
      "INFO: Iter 23800: gen_loss=5.226e+01, dis_loss=4.982e+01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m gen_loss_v\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m gen_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 22\u001b[0m gen_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mgen_loss_v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m iter_no \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iter_no \u001b[38;5;241m%\u001b[39m REPORT_EVERY_ITER \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch_v in iterate_batches(envs):\n",
    "    # generate extra fake samples, input is 4D: batch, filters, x, y\n",
    "    gen_input_v = torch.FloatTensor(BATCH_SIZE, LATENT_VECTOR_SIZE, 1, 1).normal_(0, 1).to(device)\n",
    "    batch_v = batch_v.to(device)\n",
    "    gen_output_v = net_gener(gen_input_v)\n",
    "\n",
    "    # train discriminator\n",
    "    dis_optimizer.zero_grad()\n",
    "    dis_output_true_v = net_discr(batch_v)\n",
    "    dis_output_fake_v = net_discr(gen_output_v.detach())\n",
    "    dis_loss = objective(dis_output_true_v, true_labels_v) + objective(dis_output_fake_v, fake_labels_v)\n",
    "    dis_loss.backward()\n",
    "    dis_optimizer.step()\n",
    "    dis_losses.append(dis_loss.item())\n",
    "\n",
    "    # train generator\n",
    "    gen_optimizer.zero_grad()\n",
    "    dis_output_v = net_discr(gen_output_v)\n",
    "    gen_loss_v = objective(dis_output_v, true_labels_v)\n",
    "    gen_loss_v.backward()\n",
    "    gen_optimizer.step()\n",
    "    gen_losses.append(gen_loss_v.item())\n",
    "\n",
    "    iter_no += 1\n",
    "    if iter_no % REPORT_EVERY_ITER == 0:\n",
    "        log.info(\"Iter %d: gen_loss=%.3e, dis_loss=%.3e\", iter_no, np.mean(gen_losses), np.mean(dis_losses))\n",
    "        writer.add_scalar(\"gen_loss\", np.mean(gen_losses), iter_no)\n",
    "        writer.add_scalar(\"dis_loss\", np.mean(dis_losses), iter_no)\n",
    "        gen_losses = []\n",
    "        dis_losses = []\n",
    "    if iter_no % SAVE_IMAGE_EVERY_ITER == 0:\n",
    "        writer.add_image(\"fake\", vutils.make_grid(gen_output_v.data[:64], normalize=True), iter_no)\n",
    "        writer.add_image(\"real\", vutils.make_grid(batch_v.data[:64], normalize=True), iter_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# баловство с tensorboard\n",
    "\n",
    "import torchvision\n",
    "writer = SummaryWriter()\n",
    "env = PreprocessingObservationWrapper(gym.make(\"AirRaid-v0\"))\n",
    "for i in range(0, 100):\n",
    "    writer.add_image('my_image', env.reset()[0], i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "x = range(100)\n",
    "for i in x:\n",
    "    writer.add_scalar('y=2x', i * 2, i)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    funcs = {\"sin\": math.sin, \"cos\": math.cos, \"tan\": math.tan}\n",
    "\n",
    "    for angle in range(-360, 360):\n",
    "        angle_rad = angle * math.pi / 180\n",
    "        for name, fun in funcs.items():\n",
    "            val = fun(angle_rad)\n",
    "            writer.add_scalar(name, val, angle)\n",
    "\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
