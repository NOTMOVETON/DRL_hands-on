{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-06 21:18:14.547778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-06 21:18:15.654571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import collections\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 2\n",
    "ENV = \"FrozenLake8x8-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env=ENV):\n",
    "        self.env = gym.make(env)\n",
    "        self.old_obs, _ = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, count=1000):\n",
    "        for i in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            self.rewards[(self.old_obs, action, new_obs)] = reward\n",
    "            self.transits[(self.old_obs, action)][new_obs] += 1\n",
    "            if (terminated or truncated):\n",
    "                self.old_obs, _ = self.env.reset()\n",
    "            else:\n",
    "                self.old_obs = new_obs\n",
    "\n",
    "    def calc_action_value(self, state, action):\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for target_state, count in target_counts.items():\n",
    "            reward = self.rewards[(state, action, target_state)]\n",
    "            val = reward + GAMMA*self.values[(target_state)]\n",
    "            action_value += (count/total)*val\n",
    "        return action_value\n",
    "\n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or best_value<action_value:\n",
    "                best_action = action\n",
    "                best_value = action_value\n",
    "        return best_action\n",
    "    \n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state, _ = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            #act_val = self.calc_action_value(state, action)\n",
    "            #self.values[state] = max(act_val, self.values[state]) \n",
    "            total_reward += reward\n",
    "            if (terminated or truncated):\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [self.calc_action_value(state, action) for action in range(self.env.action_space.n)]\n",
    "            self.values[state] = max(state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1\n",
      "0.0\n",
      "2\n",
      "0.0\n",
      "3\n",
      "0.0\n",
      "4\n",
      "0.0\n",
      "5\n",
      "0.0\n",
      "6\n",
      "0.0\n",
      "7\n",
      "0.0\n",
      "8\n",
      "0.0\n",
      "9\n",
      "0.0\n",
      "10\n",
      "0.0\n",
      "11\n",
      "0.0\n",
      "12\n",
      "0.0\n",
      "13\n",
      "0.0\n",
      "14\n",
      "0.0\n",
      "15\n",
      "0.0\n",
      "16\n",
      "0.0\n",
      "17\n",
      "0.0\n",
      "18\n",
      "0.0\n",
      "19\n",
      "0.0\n",
      "20\n",
      "0.0\n",
      "21\n",
      "0.0\n",
      "22\n",
      "0.0\n",
      "23\n",
      "0.0\n",
      "24\n",
      "0.0\n",
      "25\n",
      "0.0\n",
      "26\n",
      "0.0\n",
      "27\n",
      "0.0\n",
      "28\n",
      "0.0\n",
      "29\n",
      "0.0\n",
      "30\n",
      "0.0\n",
      "31\n",
      "0.0\n",
      "32\n",
      "0.0\n",
      "33\n",
      "0.0\n",
      "34\n",
      "0.0\n",
      "35\n",
      "0.0\n",
      "36\n",
      "0.0\n",
      "37\n",
      "0.0\n",
      "38\n",
      "0.0\n",
      "39\n",
      "0.0\n",
      "40\n",
      "0.0\n",
      "41\n",
      "0.0\n",
      "42\n",
      "0.0\n",
      "43\n",
      "0.0\n",
      "44\n",
      "0.0\n",
      "45\n",
      "0.0\n",
      "46\n",
      "0.0\n",
      "47\n",
      "0.0\n",
      "48\n",
      "0.0\n",
      "49\n",
      "0.0\n",
      "50\n",
      "0.0\n",
      "51\n",
      "0.0\n",
      "52\n",
      "0.0\n",
      "53\n",
      "0.0\n",
      "54\n",
      "0.0\n",
      "55\n",
      "0.0\n",
      "56\n",
      "0.0\n",
      "57\n",
      "0.0\n",
      "58\n",
      "0.0\n",
      "59\n",
      "0.0\n",
      "60\n",
      "0.0\n",
      "61\n",
      "0.0\n",
      "62\n",
      "0.0\n",
      "63\n",
      "0.0\n",
      "64\n",
      "0.0\n",
      "65\n",
      "0.0\n",
      "66\n",
      "0.0\n",
      "67\n",
      "0.0\n",
      "68\n",
      "0.0\n",
      "69\n",
      "0.0\n",
      "70\n",
      "0.0\n",
      "71\n",
      "0.0\n",
      "72\n",
      "0.0\n",
      "73\n",
      "0.0\n",
      "74\n",
      "0.0\n",
      "75\n",
      "0.0\n",
      "76\n",
      "0.0\n",
      "77\n",
      "0.0\n",
      "78\n",
      "0.0\n",
      "79\n",
      "0.0\n",
      "80\n",
      "0.0\n",
      "81\n",
      "0.0\n",
      "82\n",
      "0.0\n",
      "83\n",
      "0.0\n",
      "84\n",
      "0.0\n",
      "85\n",
      "0.0\n",
      "86\n",
      "0.0\n",
      "87\n",
      "0.0\n",
      "88\n",
      "0.0\n",
      "89\n",
      "0.0\n",
      "90\n",
      "0.0\n",
      "91\n",
      "0.0\n",
      "92\n",
      "0.0\n",
      "93\n",
      "0.0\n",
      "94\n",
      "0.0\n",
      "95\n",
      "0.0\n",
      "96\n",
      "0.0\n",
      "97\n",
      "0.0\n",
      "98\n",
      "0.0\n",
      "99\n",
      "0.0\n",
      "100\n",
      "0.0\n",
      "101\n",
      "0.0\n",
      "102\n",
      "0.0\n",
      "103\n",
      "0.0\n",
      "104\n",
      "0.0\n",
      "105\n",
      "0.0\n",
      "106\n",
      "0.0\n",
      "107\n",
      "0.0\n",
      "108\n",
      "0.0\n",
      "109\n",
      "0.0\n",
      "110\n",
      "0.0\n",
      "111\n",
      "0.0\n",
      "112\n",
      "0.0\n",
      "113\n",
      "0.0\n",
      "114\n",
      "0.0\n",
      "115\n",
      "0.0\n",
      "116\n",
      "0.0\n",
      "117\n",
      "0.0\n",
      "118\n",
      "0.0\n",
      "119\n",
      "0.0\n",
      "120\n",
      "0.0\n",
      "121\n",
      "0.0\n",
      "122\n",
      "0.0\n",
      "123\n",
      "0.0\n",
      "124\n",
      "0.0\n",
      "125\n",
      "0.0\n",
      "126\n",
      "0.0\n",
      "127\n",
      "0.0\n",
      "128\n",
      "0.0\n",
      "129\n",
      "0.0\n",
      "130\n",
      "0.0\n",
      "131\n",
      "0.0\n",
      "132\n",
      "0.0\n",
      "133\n",
      "0.0\n",
      "134\n",
      "0.0\n",
      "135\n",
      "0.0\n",
      "136\n",
      "0.0\n",
      "137\n",
      "0.0\n",
      "138\n",
      "0.0\n",
      "139\n",
      "0.0\n",
      "140\n",
      "0.0\n",
      "141\n",
      "0.0\n",
      "142\n",
      "0.0\n",
      "143\n",
      "0.0\n",
      "144\n",
      "0.0\n",
      "145\n",
      "0.0\n",
      "146\n",
      "0.0\n",
      "147\n",
      "0.0\n",
      "148\n",
      "0.0\n",
      "149\n",
      "0.0\n",
      "150\n",
      "0.0\n",
      "151\n",
      "0.0\n",
      "152\n",
      "0.0\n",
      "153\n",
      "0.0\n",
      "154\n",
      "0.0\n",
      "155\n",
      "0.0\n",
      "156\n",
      "0.0\n",
      "157\n",
      "0.0\n",
      "158\n",
      "0.0\n",
      "159\n",
      "0.0\n",
      "160\n",
      "0.0\n",
      "161\n",
      "0.0\n",
      "162\n",
      "0.0\n",
      "163\n",
      "0.0\n",
      "164\n",
      "1.0\n",
      "Best reward updated: 1.0\n",
      "V(S) = defaultdict(<class 'float'>, {8: 0.0035754388613838724, 0: 0.002513064545243277, 1: 0.006575048093283117, 2: 0.015152985108053426, 9: 0.007946030848708166, 3: 0.031170134904553913, 10: 0.01621723819226365, 11: 0.03256175265978673, 4: 0.05632590352525885, 5: 0.07409893037053286, 6: 0.08723726335446974, 14: 0.10576977420691736, 7: 0.08952827618132353, 15: 0.11380803486545699, 16: 0.004122854979346586, 17: 0.007414701759834046, 18: 0.010444619613533476, 19: 0.0, 12: 0.06634187886544578, 13: 0.08637551020381516, 22: 0.13543465461219148, 23: 0.14861001149716951, 24: 0.004590629916114762, 25: 0.007660577831909219, 26: 0.013264581401941726, 20: 0.06803488577088619, 21: 0.0898008534994918, 30: 0.17891034420489266, 31: 0.2157885905993264, 32: 0.0036427929947199147, 33: 0.004719747391509771, 27: 0.024461208832759054, 28: 0.05924591813846351, 29: 0.0, 38: 0.19574033266971802, 41: 0.0, 34: 0.005448671939694828, 35: 0.0, 36: 0.1050610686173277, 37: 0.14864648985859602, 46: 0.0, 39: 0.32055643215204455, 40: 0.0017000009757261953, 42: 0.0, 43: 0.0, 44: 0.10895622982683908, 45: 0.1159212238584458, 47: 0.9, 48: 0.0008814072742250343, 49: 0.0, 50: 0.0, 51: 0.0, 52: 0.0, 53: 0.10432910147260123, 54: 0.0, 55: 1.0, 56: 0.0007932665468025309, 57: 0.0007139398921222779, 58: 0.0, 59: 0.0, 60: 0.0, 61: 0.0938961913253411, 62: 0.0, 63: 0.0})\n",
      "WON!\n",
      "Solved in 165 iterations\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    test_env = gym.make(ENV)\n",
    "    agent = Agent()\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    agent.play_n_random_steps(100)\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        agent.play_n_random_steps(100)\n",
    "        agent.value_iteration()\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "            #agent.value_iteration()\n",
    "        reward /=  TEST_EPISODES\n",
    "        print(reward)\n",
    "        if (reward > best_reward):\n",
    "            best_reward = reward\n",
    "            print(f\"Best reward updated: {best_reward}\")\n",
    "            print(f\"V(S) = {agent.values}\")\n",
    "        \n",
    "        if (best_reward > 0.9):\n",
    "            print(\"WON!\")\n",
    "            print(f\"Solved in {iter_no} iterations\")\n",
    "            break\n",
    "        print(iter_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env=ENV):\n",
    "        self.env = gym.make(env)\n",
    "        self.old_obs, _ = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, count=1000):\n",
    "        for i in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            self.rewards[(self.old_obs, action, new_obs)] = reward\n",
    "            self.transits[(self.old_obs, action)][new_obs] += 1\n",
    "            if (terminated or truncated):\n",
    "                self.old_obs, _ = self.env.reset()\n",
    "            else:\n",
    "                self.old_obs = new_obs\n",
    "\n",
    "    # def calc_action_value(self, state, action):\n",
    "    #     target_counts = self.transits[(state, action)]\n",
    "    #     total = sum(target_counts.values())\n",
    "    #     action_value = 0.0\n",
    "    #     for target_state, count in target_counts.items():\n",
    "    #         reward = self.rewards[(state, action, target_state)]\n",
    "    #         val = reward + GAMMA*self.values[(target_state)]\n",
    "    #         action_value += (count/total)*val\n",
    "    #     return action_value\n",
    "\n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value<action_value:\n",
    "                best_action = action\n",
    "                best_value = action_value\n",
    "        return best_action\n",
    "    \n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state, _ = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            #act_val = self.calc_action_value(state, action)\n",
    "            #self.values[state] = max(act_val, self.values[state]) \n",
    "            total_reward += reward\n",
    "            if (terminated or truncated):\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                target_counts = self.transits[(state, action)]\n",
    "                total = sum(target_counts.values())\n",
    "                action_value = 0.0\n",
    "                for target_state, count in target_counts.items():\n",
    "                    reward = self.rewards[(state, action, target_state)]\n",
    "                    best_action = self.select_action(target_state)\n",
    "                    val = (count / total) * (reward + GAMMA * self.values[(target_state, best_action)])\n",
    "                    action_value += (count/total)*val\n",
    "                self.values[(state, action)] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1\n",
      "0.0\n",
      "2\n",
      "0.0\n",
      "3\n",
      "0.0\n",
      "4\n",
      "0.0\n",
      "5\n",
      "0.0\n",
      "6\n",
      "0.0\n",
      "7\n",
      "0.0\n",
      "8\n",
      "0.0\n",
      "9\n",
      "0.0\n",
      "10\n",
      "0.0\n",
      "11\n",
      "0.0\n",
      "12\n",
      "0.0\n",
      "13\n",
      "0.0\n",
      "14\n",
      "0.0\n",
      "15\n",
      "0.0\n",
      "16\n",
      "0.0\n",
      "17\n",
      "0.0\n",
      "18\n",
      "0.0\n",
      "19\n",
      "0.0\n",
      "20\n",
      "0.0\n",
      "21\n",
      "0.0\n",
      "22\n",
      "0.0\n",
      "23\n",
      "0.0\n",
      "24\n",
      "0.0\n",
      "25\n",
      "0.0\n",
      "26\n",
      "0.0\n",
      "27\n",
      "0.0\n",
      "28\n",
      "0.0\n",
      "29\n",
      "0.0\n",
      "30\n",
      "0.0\n",
      "31\n",
      "0.0\n",
      "32\n",
      "0.0\n",
      "33\n",
      "0.0\n",
      "34\n",
      "0.0\n",
      "35\n",
      "0.0\n",
      "36\n",
      "0.0\n",
      "37\n",
      "0.0\n",
      "38\n",
      "0.0\n",
      "39\n",
      "0.0\n",
      "40\n",
      "0.0\n",
      "41\n",
      "0.0\n",
      "42\n",
      "0.0\n",
      "43\n",
      "0.0\n",
      "44\n",
      "0.0\n",
      "45\n",
      "0.0\n",
      "46\n",
      "0.5\n",
      "Best reward updated: 0.5\n",
      "V(S) = defaultdict(<class 'float'>, {(0, 0): 0.0, (0, 1): 0.0, (0, 2): 0.0, (0, 3): 0.0, (4, 0): 0.0, (4, 1): 0.0, (4, 2): 0.0, (4, 3): 0.0, (1, 0): 0.0, (1, 1): 0.0, (1, 2): 0.0, (1, 3): 0.0, (2, 0): 0.0, (2, 1): 0.0, (2, 2): 0.0, (2, 3): 0.0, (5, 0): 0.0, (5, 1): 0.0, (5, 2): 0.0, (5, 3): 0.0, (6, 0): 0.0075282357752230735, (6, 1): 0.0075282357752230735, (6, 2): 0.0, (6, 3): 0.0, (3, 0): 0.0, (3, 1): 0.0, (3, 2): 0.0, (3, 3): 0.0, (7, 0): 0.0, (7, 1): 0.0, (7, 2): 0.0, (7, 3): 0.0, (8, 0): 7.037076420693976e-05, (8, 1): 4.5464257774146474e-05, (8, 2): 0.0002384282910942468, (8, 3): 0.0, (12, 0): 0.0016938530494251915, (12, 1): 0.0016938530494251915, (12, 2): 0.0016938530494251915, (12, 3): 0.0016938530494251915, (9, 0): 0.010465075006216691, (9, 1): 0.004001352208259323, (9, 2): 0.01316573952395003, (9, 3): 0.0, (10, 0): 0.036596443809948705, (10, 1): 0.0007899443714370018, (10, 2): 0.0017830032099212542, (10, 3): 0.0016938530494251915, (11, 0): 0.0016938530494251915, (11, 1): 0.0016938530494251915, (11, 2): 0.0016938530494251915, (11, 3): 0.0016938530494251915, (13, 0): 0.0021543937402827324, (13, 1): 0.050320110238679466, (13, 2): 0.020128044095471788, (13, 3): 0.004739666228622011, (14, 0): 0.006469728459258788, (14, 1): 0.22373384416756462, (14, 2): 0.08809520114097857, (14, 3): 0.010064022047735892, (15, 0): 0.010064022047735892, (15, 1): 0.010064022047735892, (15, 2): 0.010064022047735892, (15, 3): 0.010064022047735892})\n",
      "47\n",
      "0.0\n",
      "48\n",
      "0.5\n",
      "49\n",
      "0.0\n",
      "50\n",
      "0.0\n",
      "51\n",
      "0.0\n",
      "52\n",
      "0.0\n",
      "53\n",
      "0.0\n",
      "54\n",
      "0.0\n",
      "55\n",
      "0.0\n",
      "56\n",
      "0.0\n",
      "57\n",
      "0.0\n",
      "58\n",
      "0.5\n",
      "59\n",
      "0.0\n",
      "60\n",
      "0.0\n",
      "61\n",
      "0.0\n",
      "62\n",
      "0.0\n",
      "63\n",
      "0.0\n",
      "64\n",
      "0.0\n",
      "65\n",
      "0.0\n",
      "66\n",
      "0.0\n",
      "67\n",
      "0.0\n",
      "68\n",
      "0.0\n",
      "69\n",
      "0.0\n",
      "70\n",
      "0.0\n",
      "71\n",
      "0.0\n",
      "72\n",
      "0.0\n",
      "73\n",
      "0.0\n",
      "74\n",
      "0.0\n",
      "75\n",
      "0.0\n",
      "76\n",
      "0.0\n",
      "77\n",
      "0.0\n",
      "78\n",
      "0.0\n",
      "79\n",
      "0.0\n",
      "80\n",
      "0.0\n",
      "81\n",
      "0.0\n",
      "82\n",
      "0.0\n",
      "83\n",
      "0.0\n",
      "84\n",
      "0.0\n",
      "85\n",
      "0.5\n",
      "86\n",
      "0.5\n",
      "87\n",
      "0.0\n",
      "88\n",
      "0.0\n",
      "89\n",
      "0.0\n",
      "90\n",
      "0.0\n",
      "91\n",
      "0.0\n",
      "92\n",
      "0.0\n",
      "93\n",
      "0.0\n",
      "94\n",
      "0.0\n",
      "95\n",
      "0.0\n",
      "96\n",
      "0.0\n",
      "97\n",
      "0.0\n",
      "98\n",
      "0.5\n",
      "99\n",
      "0.5\n",
      "100\n",
      "0.5\n",
      "101\n",
      "0.5\n",
      "102\n",
      "0.0\n",
      "103\n",
      "0.0\n",
      "104\n",
      "0.0\n",
      "105\n",
      "0.0\n",
      "106\n",
      "0.0\n",
      "107\n",
      "0.0\n",
      "108\n",
      "0.0\n",
      "109\n",
      "0.0\n",
      "110\n",
      "0.0\n",
      "111\n",
      "0.5\n",
      "112\n",
      "0.0\n",
      "113\n",
      "0.0\n",
      "114\n",
      "0.5\n",
      "115\n",
      "0.0\n",
      "116\n",
      "0.0\n",
      "117\n",
      "0.0\n",
      "118\n",
      "0.5\n",
      "119\n",
      "0.5\n",
      "120\n",
      "0.0\n",
      "121\n",
      "0.0\n",
      "122\n",
      "0.0\n",
      "123\n",
      "0.5\n",
      "124\n",
      "0.0\n",
      "125\n",
      "0.0\n",
      "126\n",
      "0.5\n",
      "127\n",
      "0.0\n",
      "128\n",
      "0.0\n",
      "129\n",
      "0.0\n",
      "130\n",
      "0.0\n",
      "131\n",
      "0.0\n",
      "132\n",
      "1.0\n",
      "Best reward updated: 1.0\n",
      "V(S) = defaultdict(<class 'float'>, {(0, 0): 0.0, (0, 1): 0.0, (0, 2): 0.0, (0, 3): 0.0, (4, 0): 0.0, (4, 1): 0.0, (4, 2): 0.0, (4, 3): 0.0, (1, 0): 0.0, (1, 1): 0.0, (1, 2): 0.0, (1, 3): 0.0, (2, 0): 0.0, (2, 1): 0.0, (2, 2): 0.0, (2, 3): 0.0, (5, 0): 0.0, (5, 1): 0.0, (5, 2): 0.0, (5, 3): 0.0, (6, 0): 5.289939780925738e-43, (6, 1): 5.107077665041885e-43, (6, 2): 0.0, (6, 3): 0.0, (3, 0): 0.0, (3, 1): 0.0, (3, 2): 0.0, (3, 3): 0.0, (7, 0): 0.0, (7, 1): 0.0, (7, 2): 0.0, (7, 3): 0.0, (8, 0): 8.665846965769485e-44, (8, 1): 8.493553346134354e-44, (8, 2): 9.480707083417321e-44, (8, 3): 0.0, (12, 0): 1.071212805637462e-43, (12, 1): 1.071212805637462e-43, (12, 2): 1.071212805637462e-43, (12, 3): 1.071212805637462e-43, (9, 0): 4.63069826121187e-43, (9, 1): 3.6839403250258905e-43, (9, 2): 6.275814748747667e-43, (9, 3): 0.0, (10, 0): 6.822198200228365e-43, (10, 1): 8.786140648246734e-44, (10, 2): 1.2984397644090447e-43, (10, 3): 1.071212805637462e-43, (11, 0): 1.071212805637462e-43, (11, 1): 1.071212805637462e-43, (11, 2): 1.071212805637462e-43, (11, 3): 1.071212805637462e-43, (13, 0): 1.8827444246242998e-43, (13, 1): 5.83601905445278e-43, (13, 2): 2.423675676396919e-43, (13, 3): 2.4206714030883857e-43, (14, 0): 1.2678248290707764e-43, (14, 1): 4.029360812009878e-43, (14, 2): 6.9868719498890496e-43, (14, 3): 1.6413803590648445e-43, (15, 0): 1.6413803590648445e-43, (15, 1): 1.6413803590648445e-43, (15, 2): 1.6413803590648445e-43, (15, 3): 1.6413803590648445e-43})\n",
      "WON!\n",
      "Solved in 133 iterations\n",
      "EVAL REWARD: 0.0\n"
     ]
    }
   ],
   "source": [
    "TEST_EPISODES = 2\n",
    "EVAL_EPISODES = 5\n",
    "ENV = \"FrozenLake-v1\"\n",
    "if __name__=='__main__':\n",
    "    test_env = gym.make(ENV)\n",
    "    agent = Agent()\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        agent.play_n_random_steps(100)\n",
    "        agent.value_iteration()\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "            #agent.value_iteration()\n",
    "        reward /=  TEST_EPISODES\n",
    "        print(reward)\n",
    "        if (reward > best_reward):\n",
    "            best_reward = reward\n",
    "            print(f\"Best reward updated: {best_reward}\")\n",
    "            print(f\"V(S) = {agent.values}\")\n",
    "        \n",
    "        if (best_reward > 0.9):\n",
    "            print(\"WON!\")\n",
    "            print(f\"Solved in {iter_no} iterations\")\n",
    "            break\n",
    "        print(iter_no)\n",
    "    \n",
    "    test_env.close()\n",
    "    test_env = gym.make(ENV, render_mode='human')\n",
    "    reward = 0.0\n",
    "    for i in range(EVAL_EPISODES):\n",
    "        reward += agent.play_episode(test_env)\n",
    "    print(f'EVAL REWARD: {reward/EVAL_EPISODES}')\n",
    "    \n",
    "    test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
