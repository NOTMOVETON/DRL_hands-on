{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "import torch\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gymnasium.wrappers import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "PERCENTILE = 70\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(torch.nn.Module):\n",
    "    def __init__(self, input_size, ouput_size, hidden_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size, ouput_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_bathes(env, policy, batch_size, device):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs, info = env.reset()\n",
    "    sm = torch.nn.Softmax(dim=1)\n",
    "    while True: \n",
    "        obs_v = torch.tensor(np.array([obs]), dtype=torch.float32).to(device)\n",
    "        act_probs_v = sm(policy(obs_v))\n",
    "        act_probs = act_probs_v.cpu().data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        step = EpisodeStep(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "        if (terminated or truncated):\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            obs, info = env.reset()\n",
    "            if (len(batch) == batch_size):\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = np.mean(rewards)\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for reward, steps in batch:\n",
    "        if (reward >= reward_bound):\n",
    "            train_obs.extend(map(lambda step: step.observation, steps))\n",
    "            train_act.extend(map(lambda step: step.action, steps))\n",
    "    train_obs_tensor = torch.tensor(train_obs, dtype=torch.float32)\n",
    "    train_act_tensor = torch.tensor(train_act, dtype=torch.float32)\n",
    "    return train_obs_tensor, train_act_tensor, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTimeLimit(gym.Wrapper):\n",
    "    def __init__(self, env, max_episode_steps):\n",
    "        super().__init__(env)\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.current_step = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_episode_steps:\n",
    "            truncated = True\n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitriy/notebook/jupyter_notebook/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\", render_mode=\"rgb_array\", max_episode_steps=5000)\n",
    "_ = env.reset()\n",
    "obs_size = env.observation_space.shape[0]\n",
    "#act_size = env.action_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "policy = Policy(obs_size, n_actions, HIDDEN_SIZE).to(device)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=policy.parameters(), lr=0.01)\n",
    "writer = SummaryWriter(comment=\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16694/3896861031.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  train_obs_tensor = torch.tensor(train_obs, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.689, reward_mean=25.1, rw_bound=24.7\n",
      "1: loss=0.655, reward_mean=29.8, rw_bound=35.7\n",
      "2: loss=0.642, reward_mean=23.3, rw_bound=24.7\n",
      "3: loss=0.615, reward_mean=43.0, rw_bound=43.0\n",
      "4: loss=0.596, reward_mean=52.8, rw_bound=58.7\n",
      "5: loss=0.575, reward_mean=49.6, rw_bound=50.7\n",
      "6: loss=0.553, reward_mean=55.7, rw_bound=60.7\n",
      "7: loss=0.520, reward_mean=65.8, rw_bound=78.4\n",
      "8: loss=0.522, reward_mean=76.3, rw_bound=81.0\n",
      "9: loss=0.511, reward_mean=88.1, rw_bound=105.5\n",
      "10: loss=0.493, reward_mean=77.8, rw_bound=85.4\n",
      "11: loss=0.461, reward_mean=86.1, rw_bound=98.7\n",
      "12: loss=0.441, reward_mean=78.9, rw_bound=90.1\n",
      "13: loss=0.444, reward_mean=94.8, rw_bound=100.0\n",
      "14: loss=0.434, reward_mean=104.2, rw_bound=113.4\n",
      "15: loss=0.408, reward_mean=123.6, rw_bound=134.4\n",
      "16: loss=0.422, reward_mean=117.5, rw_bound=123.4\n",
      "17: loss=0.434, reward_mean=91.9, rw_bound=98.1\n",
      "18: loss=0.396, reward_mean=98.2, rw_bound=104.8\n",
      "19: loss=0.405, reward_mean=135.5, rw_bound=151.8\n",
      "20: loss=0.394, reward_mean=240.0, rw_bound=246.2\n",
      "21: loss=0.386, reward_mean=303.8, rw_bound=352.3\n",
      "22: loss=0.369, reward_mean=265.2, rw_bound=300.8\n",
      "23: loss=0.381, reward_mean=253.9, rw_bound=264.0\n",
      "24: loss=0.348, reward_mean=140.3, rw_bound=143.7\n",
      "25: loss=0.324, reward_mean=123.5, rw_bound=128.4\n",
      "26: loss=0.345, reward_mean=113.0, rw_bound=118.0\n",
      "27: loss=0.354, reward_mean=133.1, rw_bound=141.7\n",
      "28: loss=0.304, reward_mean=156.9, rw_bound=165.0\n",
      "29: loss=0.331, reward_mean=233.6, rw_bound=255.4\n",
      "30: loss=0.344, reward_mean=448.6, rw_bound=478.2\n",
      "31: loss=0.338, reward_mean=4150.8, rw_bound=5000.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "for iter_num, batch in enumerate(iterate_bathes(env, policy, BATCH_SIZE, device)):\n",
    "    obs_tensor, act_tensor, reward_bound, reward_mean = filter_batch(batch, PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_tensor = policy(obs_tensor.to(device))\n",
    "    loss_tensor = loss(action_scores_tensor, (act_tensor.long()).to(device))\n",
    "    loss_tensor.backward()\n",
    "    optimizer.step()\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
    "            iter_num, loss_tensor.item(), reward_mean, reward_bound))\n",
    "    writer.add_scalar(\"loss\", loss_tensor.item(), iter_num)\n",
    "    writer.add_scalar(\"reward_bound\", reward_bound, iter_num)\n",
    "    writer.add_scalar(\"reward_mean\", reward_mean, iter_num)\n",
    "\n",
    "    if reward_mean > 2000:\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitriy/notebook/jupyter_notebook/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/dmitriy/notebook/jupyter_notebook/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/dmitriy/ITMO/DISS/chapter_4/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0', render_mode='rgb_array', max_episode_steps=5000)\n",
    "env = RecordVideo(env, video_folder=\"./videos\", name_prefix=\"eval\",\n",
    "                  episode_trigger=lambda x: True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/dmitriy/ITMO/DISS/chapter_4/videos/eval-episode-0.mp4.\n",
      "Moviepy - Writing video /home/dmitriy/ITMO/DISS/chapter_4/videos/eval-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/dmitriy/ITMO/DISS/chapter_4/videos/eval-episode-0.mp4\n",
      "Agent failed on step: 868\n"
     ]
    }
   ],
   "source": [
    "policy.eval()\n",
    "obs, info = env.reset()\n",
    "sm = torch.nn.Softmax(dim=1)\n",
    "for i in range(5000):\n",
    "    with torch.no_grad():\n",
    "        act_probs_v = sm(policy(torch.tensor([obs], dtype=torch.float32).to(device)))\n",
    "        act_probs = act_probs_v.cpu().data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        if (terminated or truncated):\n",
    "            env.close()\n",
    "            print(f'Agent failed on step: {i}')\n",
    "            break\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
